{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "foreign-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from kornia.filters import laplacian, sobel\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latest-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SIREN needs a specific way of initialization for the\n",
    "Layers\n",
    "#code inspired from youtuber mildlyoverfitted\n",
    "'''\n",
    "\n",
    "def initializer(\n",
    "                    weight: torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor,\n",
    "                    is_first: bool = True,\n",
    "                    omega: float = 1.,\n",
    "                    c: float = 6.\n",
    "               ) -> None:\n",
    "    '''\n",
    "    Intializes the wrights of linear layer\n",
    "    Weights are based on the paper(supplimentary page 5) presented by SIREN, choose the distribution in range\n",
    "    For first layer:\n",
    "        uniform distribution in the range: (-1, 1):\n",
    "    For rest of layers:\n",
    "        bound = sqrt(c / (features_in * (omega**2)))\n",
    "        uniform distribution in the range: [-bound, bound]\n",
    "        \n",
    "    params:\n",
    "            weight: torch.FloatTensor\n",
    "            ----------------> Initialzing weight for Linear layer\n",
    "            is_first: bool\n",
    "            ----------------> If true, the layer is first layer in the network\n",
    "            omega: float\n",
    "            ----------------> hyperparameter\n",
    "            c: float\n",
    "            ---------------->hyperparameter\n",
    "    '''\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bound = 1 / weight.size()[1] if is_first else math.sqrt(c / weight.size()[1]) / omega\n",
    "        weight.uniform_(-bound, bound)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equivalent-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "class siren_layer(nn.Module):\n",
    "    '''\n",
    "    SIREN LAYER - Sinusoidal Representation Network Layer\n",
    "    params:\n",
    "        in_features: int\n",
    "            #input_features\n",
    "        ------------------\n",
    "        out_features: int\n",
    "            #out_features\n",
    "        ------------------\n",
    "        bias: bool\n",
    "            True->Needs Bias ; False->Do not Need Bias\n",
    "        ------------------\n",
    "        is_first: bool\n",
    "            True->Is the fisr layers; False->Not First Layer\n",
    "            Has influence on the initialization of the layer\n",
    "        ------------------\n",
    "        omega: flaot\n",
    "            Hyperparameter helpful in initialization\n",
    "        ------------------\n",
    "        c: int\n",
    "            Hyperparameter helful in initialization\n",
    "        ------------------\n",
    "        custom_initalizationg_function: function\n",
    "            Send custom initialization function accepting a Tensor for initializing it        \n",
    "    '''\n",
    "    def __init__(\n",
    "                    self, \n",
    "                    in_features: int,\n",
    "                    out_features: int,\n",
    "                    bias: bool = True,\n",
    "                    is_first: bool = False,\n",
    "                    omega: float = 30,\n",
    "                    c: float = 6,\n",
    "                    custom_initalizationg_function: callable = None\n",
    "                ):\n",
    "        \n",
    "        super(siren_layer, self).__init__()\n",
    "        \n",
    "        self.omega, self.c = omega, c\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "                                    in_features = in_features,\n",
    "                                    out_features = out_features,\n",
    "                                    bias = bias\n",
    "                                )\n",
    "        initializer(self.linear.weight, is_first, omega, c) if custom_initalizationg_function is None else custom_initalizationg_function(self.linear.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        sin( omega * Linear(input))\n",
    "        '''\n",
    "        ############################################\n",
    "        #           LOOK AT THIS LATER             #\n",
    "        #         Paper says omega.Wx +b           #\n",
    "        #     while the official implementation    #\n",
    "        #        does the omega(Wx + b)            #\n",
    "        ############################################\n",
    "        self.linear = self.linear.cuda() if torch.cuda.is_available() else self.linear\n",
    "#         print(self.linear.weight.size(), x.size(), x.t().size(), self.linear.bias.size())\n",
    "        return torch.sin(self.omega  * torch.mm(x, self.linear.weight.t()) + self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demanding-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_siren(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    SIREN LAYER - Image SIREN\n",
    "    Has a Sequential module inside the class\n",
    "    params:\n",
    "        hidden_layers: int\n",
    "            #hidden_layers\n",
    "        ------------------\n",
    "        hidden_features: int\n",
    "            #hidden_features\n",
    "        ------------------\n",
    "        first_omega: flaot\n",
    "            Hyperparameter helpful in initialization\n",
    "        ------------------\n",
    "        hidden_omega: flaot\n",
    "            Hyperparameter helpful in initialization\n",
    "        ------------------\n",
    "        c: int\n",
    "            Hyperparameter helful in initialization\n",
    "        ------------------\n",
    "        custom_initalizationg_function: function\n",
    "            Send custom initialization function accepting a Tensor for initializing it        \n",
    "    '''\n",
    "    def __init__(\n",
    "                    self,\n",
    "                    hidden_layers: int = 1,\n",
    "                    hidden_features: int = 10,\n",
    "                    first_omega: float = 30,\n",
    "                    hidden_omega: float = 30,\n",
    "                    c: float = 6,\n",
    "                    custom_initalizationg_function: callable = None\n",
    "              ):\n",
    "        \n",
    "        super(image_siren, self).__init__()\n",
    "\n",
    "        in_features, out_features = 2, 1\n",
    "        network = []\n",
    "        \n",
    "        network.append(\n",
    "                            # First layer\n",
    "                            siren_layer(\n",
    "                                            in_features, #Image coordinates\n",
    "                                            hidden_features,\n",
    "                                            is_first=True, # For a different Initialization\n",
    "                                            custom_initalizationg_function=custom_initalizationg_function,\n",
    "                                            omega=first_omega,\n",
    "                                            c=c,\n",
    "                                            bias=True\n",
    "                                       )\n",
    "                      )\n",
    "        for _ in range(hidden_layers):\n",
    "            network.append(\n",
    "                            # Hidden Layes\n",
    "                            siren_layer(\n",
    "                                            hidden_features, #Hidden Features\n",
    "                                            hidden_features,\n",
    "                                            is_first=False, # For a different Initialization\n",
    "                                            custom_initalizationg_function=custom_initalizationg_function,\n",
    "                                            omega=hidden_omega,\n",
    "                                            c=c,\n",
    "                                            bias=True\n",
    "                                       )\n",
    "                      )\n",
    "    \n",
    "        #FINAL LAYER\n",
    "        network.append( \n",
    "                            nn.Linear(hidden_features, out_features)\n",
    "                      )\n",
    "        \n",
    "        initializer(network[-1].weight, False, hidden_omega, c) if custom_initalizationg_function is None else custom_initalizationg_function(network[-1].weight)\n",
    "\n",
    "        # MODEL\n",
    "        self.model = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "invalid-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from functools import partial\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "### VARIOUS INIT FUNCTIONS ###\n",
    "init_functions = {\n",
    "                    'ones':torch.nn.init.ones_,\n",
    "                    'eye':torch.nn.init.eye_,\n",
    "                    'default': partial(torch.nn.init.kaiming_uniform_, a=5 ** (.5)),\n",
    "                    'paper': None\n",
    "                 }\n",
    "\n",
    "for init_name, init_function in init_functions.items():\n",
    "    path = pathlib.Path.cwd() / 'tensorboard_logs' / init_name\n",
    "    writer = SummaryWriter(log_dir=path)\n",
    "    \n",
    "    def layer_logger(inst, inp, out, number=0):\n",
    "        layer_name = f\"{number}_{inst.__class__.__name__}\"\n",
    "        writer.add_histogram(layer_name, out)\n",
    "    \n",
    "    model = image_siren(\n",
    "                            hidden_layers=10,\n",
    "                            hidden_features=200,\n",
    "                            first_omega=30,\n",
    "                            hidden_omega=30,\n",
    "                            c=6,\n",
    "                            custom_initalizationg_function=init_function\n",
    "                        )\n",
    "    model = model.cuda() if torch.cuda.is_available() else model\n",
    "    \n",
    "    for i, layer in enumerate(model.model.modules()):\n",
    "        if not i:\n",
    "            continue\n",
    "        layer.register_forward_hook(partial(layer_logger, number=(i + 1) // 2))\n",
    "    \n",
    "    inp = 2 * (torch.rand(10000, 2) - .5)\n",
    "    \n",
    "    inp = inp.cuda() if torch.cuda.is_available() else inp\n",
    "    \n",
    "    writer.add_histogram('0', inp)\n",
    "    \n",
    "    res = model(inp)\n",
    "    \n",
    "    del model, inp\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-robert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-howard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
